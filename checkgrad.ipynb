{
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language": "lua",
  "language_info": {
   "name": "lua",
   "version": "20100"
  },
  "name": "",
  "signature": "sha256:9a8a3cdea7b8e74d2c617e669aae92e53a68b57088f24d429ea3c46fa550d6ac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "require 'torch'\n",
      "torch.setdefaulttensortype('torch.FloatTensor')\n",
      "require 'nn'\n",
      "require 'nngraph'\n",
      "require 'optim'\n",
      "require 'parsevocab'\n",
      "local LSTMH1 = require 'LSTMH1'\n",
      "local LSTMHN = require 'LSTMHN'\n",
      "require 'windowMat2'\n",
      "require 'yHatMat2'\n",
      "require 'mixtureCriterionMat'\n",
      "local model_utils=require 'model_utils'\n",
      "require 'cunn'\n",
      "require 'distributions'\n",
      "torch.manualSeed(123)\n",
      "\n",
      "opt = {}\n",
      "opt.maxlen = 2\n",
      "opt.batchSize = 2\n",
      "opt.numPasses = 1\n",
      "opt.lr = 1e-3\n",
      "opt.inputSize = 3\n",
      "opt.hiddenSize = 2\n",
      "\n",
      "\n",
      "-- get training dataset\n",
      "--dataFile = torch.DiskFile('data_train.asc', 'r')\n",
      "dataFile = torch.DiskFile('data_norm_mean_toy.asc', 'r')\n",
      "handwritingdata = dataFile:readObject()\n",
      "dataSize = #handwritingdata\n",
      "\n",
      "print('Uploaded training')\n",
      "\n",
      "-- get validation dataset\n",
      "--valdataFile = torch.DiskFile('data_valid.asc', 'r')\n",
      "valdataFile = torch.DiskFile('data_norm_mean_toy.asc', 'r')\n",
      "valhandwritingdata = valdataFile:readObject()\n",
      "valdataSize = #valhandwritingdata\n",
      "\n",
      "print('Uploaded validation')\n",
      "\n",
      "-- make model\n",
      "model = {}\n",
      "\n",
      "model.criterion = nn.MixtureCriterion():cuda()\n",
      "model.criterion:setSizeAverage()\n",
      "\n",
      "\n",
      "local input_xin = nn.Identity()()\n",
      "local input_context = nn.Identity()()\n",
      "local input_w_prev = nn.Identity()()\n",
      "local input_lstm_h1_h = nn.Identity()()\n",
      "local input_lstm_h1_c = nn.Identity()()\n",
      "local input_lstm_h2_h = nn.Identity()()\n",
      "local input_lstm_h2_c = nn.Identity()()\n",
      "local input_lstm_h3_h = nn.Identity()()\n",
      "local input_lstm_h3_c = nn.Identity()()\n",
      "local input_prev_kappa = nn.Identity()()\n",
      "\n",
      "local h1 = LSTMH1.lstm(opt.inputSize, opt.hiddenSize)({input_xin, input_w_prev, input_lstm_h1_c, input_lstm_h1_h})\n",
      "local h1_c = nn.SelectTable(1)(h1)\n",
      "local h1_h = nn.SelectTable(2)(h1)\n",
      "local w_output = nn.Window()({nn.Linear(opt.hiddenSize,30)(h1_h), input_context, input_prev_kappa})\n",
      "local w_vector = nn.SelectTable(1)(w_output)\n",
      "local w_kappas_t = nn.SelectTable(2)(w_output)\n",
      "local w_phi_t = nn.SelectTable(3)(w_output)\n",
      "local h2 = LSTMHN.lstm(opt.inputSize, opt.hiddenSize)({input_xin, w_vector, h1_h, input_lstm_h2_c, input_lstm_h2_h})\n",
      "local h2_c = nn.SelectTable(1)(h2)\n",
      "local h2_h = nn.SelectTable(2)(h2)\n",
      "local h3 = LSTMHN.lstm(opt.inputSize, opt.hiddenSize)({input_xin, w_vector, h2_h, input_lstm_h3_c, input_lstm_h3_h})\n",
      "local h3_c = nn.SelectTable(1)(h3)\n",
      "local h3_h = nn.SelectTable(2)(h3)\n",
      "local y = nn.YHat()(nn.Linear(opt.hiddenSize*3, 121)(nn.JoinTable(2)({h1_h, h2_h, h3_h})))\n",
      "\n",
      "model.rnn_core = nn.gModule({input_xin, input_context, input_prev_kappa, input_w_prev,  \n",
      "                             input_lstm_h1_c, input_lstm_h1_h,\n",
      "                             input_lstm_h2_c, input_lstm_h2_h,\n",
      "                             input_lstm_h3_c, input_lstm_h3_h},\n",
      "                            {y, w_kappas_t, w_vector, w_phi_t, h1_c, h1_h, h2_c, h2_h,\n",
      "                             h3_c, h3_h})\n",
      "\n",
      "model.rnn_core:cuda()\n",
      "params, grad_params = model.rnn_core:getParameters()\n",
      "\n",
      "--params, grad_params = model_utils.combine_all_parameters(model.h1, model.h1_w, model.w, model.h2, model.h3, model.h3_y, model.y)\n",
      "--params:uniform(-0.08, 0.08)\n",
      "\n",
      "-- LSTM initial state (zero initially, but final state gets sent to initial state when we do BPTT)\n",
      "initstate_h1_c = torch.zeros(1, opt.hiddenSize):cuda()\n",
      "initstate_h1_h = initstate_h1_c:clone()\n",
      "initstate_h2_c = initstate_h1_c:clone()\n",
      "initstate_h2_h = initstate_h1_c:clone()\n",
      "\n",
      "-- LSTM final state's backward message (dloss/dfinalstate) is 0, since it doesn't influence predictions\n",
      "dfinalstate_h1_c = initstate_h1_c:clone()\n",
      "dfinalstate_h1_h = initstate_h1_c:clone()\n",
      "dfinalstate_h2_c = initstate_h1_c:clone()\n",
      "dfinalstate_h2_h = initstate_h1_c:clone()\n",
      "\n",
      "-- make a bunch of clones, AFTER flattening, as that reallocates memory\n",
      "MAXLEN = opt.maxlen\n",
      "clones = {} -- TODO: local\n",
      "for name,mod in pairs(model) do\n",
      "    print('cloning '..name)\n",
      "    clones[name] = model_utils.clone_many_times_fast(mod, MAXLEN-1, not mod.parameters)\n",
      "end\n",
      "print('start training')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "Uploaded training\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "Uploaded validation\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "cloning criterion\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "cloning rnn_core\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "start training\t\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "require 'getBatch'\n",
      "params:uniform(-0.08, 0.08)\n",
      "sampleSize = opt.batchSize\n",
      "numberOfPasses = opt.numPasses\n",
      "\n",
      "-- LSTM initial state (zero initially, but final state gets sent to initial state when we do BPTT)\n",
      "initstate_h1_c = torch.zeros(sampleSize, opt.hiddenSize):cuda()\n",
      "initstate_h1_h = initstate_h1_c:clone()\n",
      "initstate_h2_c = initstate_h1_c:clone()\n",
      "initstate_h2_h = initstate_h1_c:clone()\n",
      "initstate_h3_c = initstate_h1_c:clone()\n",
      "initstate_h3_h = initstate_h1_c:clone()\n",
      "\n",
      "-- LSTM final state's backward message (dloss/dfinalstate) is 0, since it doesn't influence predictions\n",
      "dfinalstate_h1_c = initstate_h1_c:clone()\n",
      "dfinalstate_h1_h = initstate_h1_c:clone()\n",
      "dfinalstate_h2_c = initstate_h1_c:clone()\n",
      "dfinalstate_h2_h = initstate_h1_c:clone()\n",
      "dfinalstate_h3_c = initstate_h1_c:clone()\n",
      "dfinalstate_h3_h = initstate_h1_c:clone()\n",
      "initkappa = torch.randn(sampleSize,10)\n",
      "dinitkappa = torch.zeros(sampleSize,10)\n",
      "\n",
      "count = 1\n",
      "\n",
      "batchCount = nil\n",
      "\n",
      "function getInitW(cuMat)\n",
      "    return cuMat[{{},{1},{}}]:squeeze(2)\n",
      "end\n",
      "\n",
      "function makecov(std, rho)\n",
      "    covmat = torch.Tensor(2,2)\n",
      "    covmat[{{1},{1}}] = torch.pow(std[{{1},{1}}], 2)\n",
      "    covmat[{{1},{2}}] = torch.cmul(torch.cmul(std[{{1},{1}}], std[{{1},{2}}]), rho[{{1},{2}}])\n",
      "    covmat[{{2},{1}}] = torch.cmul(torch.cmul(std[{{1},{1}}], std[{{1},{2}}]), rho[{{1},{2}}])\n",
      "    covmat[{{2},{2}}] = torch.pow(std[{{1},{2}}], 2)\n",
      "    return covmat\n",
      "end\n",
      "\n",
      "function getSample(sampleSize, yOutput)\n",
      "    sampX = torch.zeros(sampleSize, 3)\n",
      "    for i=1,sampleSize do\n",
      "        currentY = yOutput[{{i},{}}]\n",
      "        x_1, x_2, x_3 = _getSample(currentY)\n",
      "        sampX[{{i},{1}}] = x_1\n",
      "        sampX[{{i},{2}}] = x_2\n",
      "        sampX[{{i},{3}}] = x_3\n",
      "    end\n",
      "    return sampX:cuda()\n",
      "end\n",
      "\n",
      "function _getSample(input)\n",
      "    e_t = input[{{},{1}}]\n",
      "    pi_t = input[{{},{2,21}}]\n",
      "    mu_1_t = input[{{},{22,41}}]\n",
      "    mu_2_t = input[{{},{42,61}}]\n",
      "    sigma_1_t = input[{{},{62,81}}]\n",
      "    sigma_2_t = input[{{},{82,101}}]\n",
      "    rho_t = input[{{},{102,121}}]\n",
      "    \n",
      "    x_3 = torch.Tensor(1)\n",
      "    x_3 = (x_3:bernoulli(e_t:squeeze())):squeeze()\n",
      "    \n",
      "    chosen_pi = torch.multinomial(pi_t:double(), 1):squeeze()\n",
      "\n",
      "    curstd = torch.Tensor({{sigma_1_t[{{},{chosen_pi}}]:squeeze(), sigma_2_t[{{},{chosen_pi}}]:squeeze()}})\n",
      "    curcor = torch.Tensor({{1, rho_t[{{},{chosen_pi}}]:squeeze()}})\n",
      "    curcovmat = makecov(curstd, curcor)\n",
      "    curmean = torch.Tensor({{mu_1_t[{{},{chosen_pi}}]:squeeze(), mu_2_t[{{},{chosen_pi}}]:squeeze()}})\n",
      "    sample = distributions.mvn.rnd(curmean, curcovmat)\n",
      "    x_1 = sample[1]\n",
      "    x_2 = sample[2]\n",
      "    return x_1, x_2, x_3\n",
      "end\n",
      "\n",
      "function schedSampBool() \n",
      "    k = 0.9\n",
      "    i = batchCount/80.0\n",
      "    e_i = k^i\n",
      "    -- if we get 1 then don't sample, if 0 then do sample\n",
      "    randvar = torch.Tensor(1)\n",
      "    result = randvar:bernoulli(e_i):squeeze()\n",
      "    return result  \n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxLen, strs, inputMat, cuMat, ymaskMat, wmaskMat, cmaskMat, elementCount, \n",
      "        count = getBatch(count, handwritingdata, sampleSize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function feval(x)\n",
      "    if x ~= params then\n",
      "        params:copy(x)\n",
      "    end\n",
      "    grad_params:zero()\n",
      "    \n",
      "    local loss = 0\n",
      "    local elems = 0\n",
      "    \n",
      "    -- add for loop to increase mini-batch size\n",
      "    for i=1, numberOfPasses do\n",
      "\n",
      "        --------------------- get mini-batch -----------------------\n",
      "--         maxLen, strs, inputMat, cuMat, ymaskMat, wmaskMat, cmaskMat, elementCount, \n",
      "--         count = getBatch(count, handwritingdata, sampleSize)\n",
      "        ------------------------------------------------------------\n",
      "\n",
      "        if maxLen > MAXLEN then\n",
      "            maxLen = MAXLEN\n",
      "        end\n",
      "\n",
      "        -- initialize window to first char in all elements of the batch\n",
      "        local w = {[0]=getInitW(cuMat:cuda())}\n",
      "\n",
      "        local lstm_c_h1 = {[0]=initstate_h1_c} -- internal cell states of LSTM\n",
      "        local lstm_h_h1 = {[0]=initstate_h1_h} -- output values of LSTM\n",
      "        local lstm_c_h2 = {[0]=initstate_h2_c} -- internal cell states of LSTM\n",
      "        local lstm_h_h2 = {[0]=initstate_h2_h} -- output values of LSTM\n",
      "        local lstm_c_h3 = {[0]=initstate_h3_c} -- internal cell states of LSTM\n",
      "        local lstm_h_h3 = {[0]=initstate_h3_h} -- output values of LSTM\n",
      "        \n",
      "        local kappa_prev = {[0]=torch.zeros(sampleSize,10):cuda()}\n",
      "        \n",
      "        local output_h1_w = {}\n",
      "        local input_h3_y = {}\n",
      "        local output_h3_y = {}\n",
      "        local output_y = {}\n",
      "        \n",
      "        -- forward\n",
      "        \n",
      "        --print('forward')\n",
      "        \n",
      "        for t = 1, maxLen - 1 do\n",
      "            local x_in = inputMat[{{},{},{t}}]:squeeze(3)\n",
      "            local x_target = inputMat[{{},{},{t+1}}]:squeeze(3)\n",
      "       \n",
      "            -- Using Scheduled Sampling\n",
      "            -- if returns 1 then don't sample, o.w. do\n",
      "            --sampleBool = schedSampBool()\n",
      "\n",
      "            --if sampleBool == 0 and t ~= 1 then\n",
      "            --    x_in = getSample(sampleSize, output_y[t-1])\n",
      "            --end\n",
      "\n",
      "            -- model \n",
      "            output_y[t], kappa_prev[t], w[t], _, lstm_c_h1[t], lstm_h_h1[t],\n",
      "            lstm_c_h2[t], lstm_h_h2[t], lstm_c_h3[t], lstm_h_h3[t]\n",
      "        = unpack(clones.rnn_core[t]:forward({x_in:cuda(), cuMat:cuda(), \n",
      "                 kappa_prev[t-1], w[t-1], lstm_c_h1[t-1], lstm_h_h1[t-1],\n",
      "                 lstm_c_h2[t-1], lstm_h_h2[t-1], lstm_c_h3[t-1], lstm_h_h3[t-1]}))\n",
      "       \n",
      "            -- criterion \n",
      "            --print(cmaskMat:clone():sum(3):squeeze())\n",
      "            clones.criterion[t]:setmask(cmaskMat[{{},{},{t}}]:cuda())\n",
      "            loss = clones.criterion[t]:forward(output_y[t], x_target:cuda()) + loss\n",
      "            --print('inner loop ',loss)        \n",
      "        end\n",
      "        --print('current pass ',loss)        \n",
      "        elems = (elementCount - sampleSize) + elems\n",
      "        \n",
      "        -- backward\n",
      "        \n",
      "        --print('backward')\n",
      "        \n",
      "        local dlstm_c_h1 = dfinalstate_h1_c\n",
      "        local dlstm_h_h1 = dfinalstate_h1_h\n",
      "        local dlstm_c_h2 = dfinalstate_h2_c\n",
      "        local dlstm_h_h2 = dfinalstate_h2_h\n",
      "        local dlstm_c_h3 = dfinalstate_h3_c\n",
      "        local dlstm_h_h3 = dfinalstate_h3_h\n",
      "        \n",
      "        local dh1_w = torch.zeros(sampleSize, 57):cuda()\n",
      "        local dkappa = torch.zeros(sampleSize, 10):cuda()\n",
      "        \n",
      "        for t = maxLen - 1, 1, -1 do\n",
      "        \n",
      "            local x_in = inputMat[{{},{},{t}}]:squeeze()\n",
      "            local x_target = inputMat[{{},{},{t+1}}]:squeeze()\n",
      "            \n",
      "            -- criterion\n",
      "            local grad_crit = clones.criterion[t]:backward(output_y[t], x_target:cuda())\n",
      "\t    grad_crit:clamp(-100,100)            \n",
      "\n",
      "            -- model\n",
      "            _x, _c, dkappa, dh1_w, dlstm_c_h1, dlstm_h_h1,\n",
      "            dlstm_c_h2, dlstm_h_h2, dlstm_c_h3, dlstm_h_h3 = unpack(clones.rnn_core[t]:backward({x_in:cuda(), cuMat:cuda(), \n",
      "                 kappa_prev[t-1], w[t-1], lstm_c_h1[t-1], lstm_h_h1[t-1],\n",
      "                 lstm_c_h2[t-1], lstm_h_h2[t-1], lstm_c_h3[t-1], lstm_h_h3[t-1]},\n",
      "                 {grad_crit, dkappa, dh1_w, _, dlstm_c_h1, dlstm_h_h1, \n",
      "                  dlstm_c_h2, dlstm_h_h2, dlstm_c_h3, dlstm_h_h3 }))\n",
      "\n",
      "        end\n",
      "    \n",
      "        dh2_w = nil\n",
      "        dh2_h1 = nil\n",
      "        dh3_w = nil\n",
      "        dh3_h2 = nil\n",
      "\n",
      "        w = nil\n",
      "        lstm_c_h1 = nil -- internal cell states of LSTM\n",
      "        lstm_h_h1 = nil -- output values of LSTM\n",
      "        lstm_c_h2 = nil -- internal cell states of LSTM\n",
      "        lstm_h_h2 = nil -- output values of LSTM\n",
      "        lstm_c_h3 = nil -- internal cell states of LSTM\n",
      "        lstm_h_h3 = nil -- output values of LSTM\n",
      "        dlstm_c_h1 = nil -- internal cell states of LSTM\n",
      "        dlstm_h_h1 = nil -- internal cell states of LSTM\n",
      "        dlstm_c_h2 = nil -- internal cell states of LSTM\n",
      "        dlstm_h_h2 = nil -- internal cell states of LSTM\n",
      "        dlstm_c_h3 = nil -- internal cell states of LSTM\n",
      "        dlstm_h_h3 = nil -- internal cell states of LSTM\n",
      "        dkappaNext = nil\n",
      "        dh1_w_next = nil\n",
      "        kappa_prev = nil\n",
      "        output_h1_w = nil\n",
      "        input_h3_y = nil\n",
      "        output_h3_y = nil\n",
      "        output_y = nil\n",
      "        collectgarbage()\n",
      "    end\n",
      "    \n",
      "    grad_params:div(numberOfPasses)\n",
      "    \n",
      "    -- clip gradient element-wise\n",
      "    \n",
      "    \n",
      "    return loss, grad_params:clone():double()\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(params:size())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "\n",
        " 2545\n",
        "[torch.LongStorage of size 1]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cost, grad = feval(params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(cost)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "0.3390856385231\t\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diff,dC,dC_est = optim.checkgrad(feval, params, 1e-4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(diff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "0.9606065576483\t\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dC_est:mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dC:mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "0.0002288162591181\t\n",
        "0.0045991770723834\t\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(cost)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "0.3390856385231\t\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}